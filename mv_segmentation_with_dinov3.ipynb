{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "TfQ7Ravw_Jo1",
   "metadata": {
    "id": "TfQ7Ravw_Jo1"
   },
   "source": [
    "# Mitral Valve (MV) segmentation in echocardiography videos using DINOv3\n",
    "\n",
    "This notebook adapts the DINOv3 segmentation-tracking propagation approach to the MV segmentation task you described.\n",
    "\n",
    "What it does:\n",
    "- Loads training videos (train.pkl) and test videos (test.pkl / test list).\n",
    "- Uses the 3 annotated frames per training video as context to propagate dense masks to every frame in the same video using DINOv3 patch features and a non-parametric propagation (top-k softmax weighting inside a local neighborhood).\n",
    "- Demonstrates how to apply the bounding box, convert grayscale to RGB, handle resizing to the DINO patch grid, upsample predictions, map back to original resolution and save masks or pseudo-labels.\n",
    "- (Optional) Provides a skeleton to train a segmentation model (UNet) on the pseudo-labels.\n",
    "\n",
    "Run sections sequentially. Replace paths to train.pkl/test.pkl with your dataset paths when needed."
   ]
  },
  {
   "cell_type": "code",
   "id": "1cgoKGC3_Jo2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1cgoKGC3_Jo2",
    "outputId": "03ae4819-6616-4b45-da44-a6e3e333b703"
   },
   "source": [
    "# Basic imports and configuration\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torchvision.transforms as TVT\n",
    "import torchvision.transforms.functional as TVTF\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', DEVICE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "Jj2HBjCf_Jo3",
   "metadata": {
    "id": "Jj2HBjCf_Jo3"
   },
   "source": [
    "## Settings: models & hyperparameters\n",
    "You can change these to fit your GPU and data. Start with smaller SHORT_SIDE if memory is tight."
   ]
  },
  {
   "cell_type": "code",
   "id": "YSeOHocn_Jo3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YSeOHocn_Jo3",
    "outputId": "33e3d501-ab77-401c-cd78-7fd40f85217d"
   },
   "source": [
    "# Configurable parameters\n",
    "DINOV3_LOCATION = os.getenv('DINOV3_LOCATION', 'facebookresearch/dinov3')  # or local repo path\n",
    "MODEL_NAME = 'dinov3_vitl16'  # try 'dinov3_vits16' if GPU constrained\n",
    "SHORT_SIDE = 640  # try 640/800/960\n",
    "TOPK = 10\n",
    "TEMPERATURE = 0.2\n",
    "NEIGHBORHOOD_PATCH_RADIUS = 16  # measured in patches; controls search radius\n",
    "USE_BBOX_CROP = True  # apply provided bbox to reduce area\n",
    "ASSUME_BINARY = True  # assume labels are background / mitral valve (0/1)\n",
    "SAVE_PSEUDOLABELS_DIR = 'pseudo_labels'\n",
    "os.makedirs(SAVE_PSEUDOLABELS_DIR, exist_ok=True)\n",
    "print('Config: SHORT_SIDE=', SHORT_SIDE, 'TOPK=', TOPK, 'NEIGHBORHOOD_RADIUS=', NEIGHBORHOOD_PATCH_RADIUS)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "xWJEmk3m_Jo3",
   "metadata": {
    "id": "xWJEmk3m_Jo3"
   },
   "source": [
    "## Load DINOv3 model\n",
    "This uses torch.hub. If you have a local repo checkout set DINOV3_LOCATION env var or change the variable above."
   ]
  },
  {
   "cell_type": "code",
   "id": "4oop9wpe_ock",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4oop9wpe_ock",
    "outputId": "243aabac-be9a-4cd8-a7c9-5dd2438ce0d0"
   },
   "source": [
    "!pip install torchmetrics"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "w0oHYOAID9ti",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w0oHYOAID9ti",
    "outputId": "8d85027a-ed14-49b6-8b03-fedba47a67bd"
   },
   "source": [
    "!wget https://dinov3.llamameta.net/dinov3_vitl16/dinov3_vitl16_pretrain_lvd1689m-8aa4cbdd.pth?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoia3Blb3kxMGhocDJ0NjduaHIxMm8wdXg5IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZGlub3YzLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NjUwMTkzNDJ9fX1dfQ__&Signature=Jp9sdIhUsBzvm8MgUhYgTBE9T0Uo%7Ex21c3ZMOZzO8lWcD1NxNrG7%7Excg2tCU-O-jOyTyFoKetX0XksD3%7EKGwAS5bMRYCQYx-ifp7ahUttS0zWa3gY2UbyEAP6NZIHvQilgYi2ZRe4ypNIVCcFlkuNsEdSHCZxsdxYCIf8gyTvRmtWYG9w9dEHYNTQQXv8ybjTfMsxnItTnQXHW47Q0uCO177JfyjUUlUKPTORvFwtr8rKjUZODWElLC-31kMuGjvjyKZD2bn%7E0Okm1q7-kSYqnItGnqJB0uG5WrCSZTfgfvfJDyU9EGkrZXAZ99qEOmFhzr8PPXXn0Dz1QSpiehyug__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1498136187968196"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "sQqG7fOUGVNF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sQqG7fOUGVNF",
    "outputId": "a922a6b1-d249-43d4-c707-64f9cbc62588"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "RzyeQOfnGbEq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RzyeQOfnGbEq",
    "outputId": "97cd9837-da6d-40a1-e078-80c2129ba437"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "T-NgmqpkFHVs",
   "metadata": {
    "id": "T-NgmqpkFHVs"
   },
   "source": [
    "#!git clone https://github.com/facebookresearch/dinov3.git\n",
    "REPO_DIR = \"/content/dinov3\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "wGK3trpqHWpW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wGK3trpqHWpW",
    "outputId": "3b16ca06-0c21-4be3-b5e0-8a726696d7b3"
   },
   "source": [
    "!wget -c \"https://dinov3.llamameta.net/dinov3_vitb16/dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoia3Blb3kxMGhocDJ0NjduaHIxMm8wdXg5IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZGlub3YzLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NjUwMTkzNDJ9fX1dfQ__&Signature=Jp9sdIhUsBzvm8MgUhYgTBE9T0Uo%7Ex21c3ZMOZzO8lWcD1NxNrG7%7Excg2tCU-O-jOyTyFoKetX0XksD3%7EKGwAS5bMRYCQYx-ifp7ahUttS0zWa3gY2UbyEAP6NZIHvQilgYi2ZRe4ypNIVCcFlkuNsEdSHCZxsdxYCIf8gyTvRmtWYG9w9dEHYNTQQXv8ybjTfMsxnItTnQXHW47Q0uCO177JfyjUUlUKPTORvFwtr8rKjUZODWElLC-31kMuGjvjyKZD2bn%7E0Okm1q7-kSYqnItGnqJB0uG5WrCSZTfgfvfJDyU9EGkrZXAZ99qEOmFhzr8PPXXn0Dz1QSpiehyug__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1498136187968196\" \\\n",
    "      -O \"/content/dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!wget -c \"https://dinov3.llamameta.net/dinov3_vitl16/dinov3_vitl16_pretrain_lvd1689m-8aa4cbdd.pth?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoia3Blb3kxMGhocDJ0NjduaHIxMm8wdXg5IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZGlub3YzLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NjUwMTkzNDJ9fX1dfQ__&Signature=Jp9sdIhUsBzvm8MgUhYgTBE9T0Uo%7Ex21c3ZMOZzO8lWcD1NxNrG7%7Excg2tCU-O-jOyTyFoKetX0XksD3%7EKGwAS5bMRYCQYx-ifp7ahUttS0zWa3gY2UbyEAP6NZIHvQilgYi2ZRe4ypNIVCcFlkuNsEdSHCZxsdxYCIf8gyTvRmtWYG9w9dEHYNTQQXv8ybjTfMsxnItTnQXHW47Q0uCO177JfyjUUlUKPTORvFwtr8rKjUZODWElLC-31kMuGjvjyKZD2bn%7E0Okm1q7-kSYqnItGnqJB0uG5WrCSZTfgfvfJDyU9EGkrZXAZ99qEOmFhzr8PPXXn0Dz1QSpiehyug__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1498136187968196\" \\\n",
    "      -O \"/content/dinov3_vitl16_pretrain_lvd1689m-8aa4cbdd.pth\"\n"
   ],
   "metadata": {
    "id": "RCmi54wUmiZl",
    "outputId": "fa4d0ced-cb4a-4524-9cb9-061d8ab19fb3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "RCmi54wUmiZl",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "jKFNVg9knp6j"
   },
   "id": "jKFNVg9knp6j",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "w-oBhYB3M6hz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w-oBhYB3M6hz",
    "outputId": "368a7bcd-3f27-46f9-d621-93f47fe5a842"
   },
   "source": [
    "!ls -lh /content/dinov3_vitl16_pretrain_lvd1689m-8aa4cbdd.pth\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5bBzqHJNAld",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5bBzqHJNAld",
    "outputId": "d65b3e9d-77e9-4f24-a81f-22fc13283f71"
   },
   "source": [
    "!head -c 200 /content/dinov3_vitl16_pretrain_lvd1689m-8aa4cbdd.pth\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "QRrlMUueNcjU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QRrlMUueNcjU",
    "outputId": "b297743f-c118-4e08-c350-787efd277e53"
   },
   "source": [
    "!unzip -t /content/dinov3_vitl16_pretrain_lvd1689m-*.pth"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "n8b5dURlE8Ox",
   "metadata": {
    "id": "n8b5dURlE8Ox"
   },
   "source": [
    "WEIGHTS_PATH = \"/content/drive/MyDrive/dinov3_vitl16_pretrain_lvd1689m-8aa4cbdd.pth\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/facebookresearch/dinov3.git"
   ],
   "metadata": {
    "id": "l6k_CtsFn7cf",
    "outputId": "c06606a2-9f1c-4449-d7d7-ad990cf00b34",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "l6k_CtsFn7cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8QWlSHDZ_Jo4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QWlSHDZ_Jo4",
    "outputId": "47b1c5d6-71a8-4d9d-d10d-9703d4577d13"
   },
   "source": [
    "import time\n",
    "\n",
    "print('Loading DINOv3 model (this may download weights)...')\n",
    "try:\n",
    "    # Correctly load the model from the local repository and local weights\n",
    "    model = torch.hub.load(\n",
    "        REPO_DIR, # Path to the local dinov3 repository\n",
    "        \"dinov3_vitl16\", # The model entry point (e.g., 'dinov3_vitl16')\n",
    "        source='local', # Indicate that the source is a local repository\n",
    "        weights=WEIGHTS_PATH # Pass the path to your locally downloaded weights using 'weights' (plural)\n",
    "    )\n",
    "except Exception as e:\n",
    "    if \"HTTP Error 403: Forbidden\" in str(e) or \"GatedRepoError\" in str(e):\n",
    "        print(\"Error: Failed to download DINOv3 model weights due to a 403 Forbidden error or Gated Repo access.\")\n",
    "        print(\"This usually means the server denied access to the download link or the model is gated.\")\n",
    "        print(\"Please ensure you have access to the model, or consider manually downloading the model weights.\")\n",
    "        print(f\"The expected download URL was: https://dl.fbaipublicfiles.com/dinov3/{MODEL_NAME}/{MODEL_NAME}_pretrain_lvd1689m-8aa4cbdd.pth\")\n",
    "    else:\n",
    "        print(f\"An unexpected error occurred during model loading: {e}\")\n",
    "    raise # Re-raise the exception to stop execution if model loading is critical\n",
    "\n",
    "model.to(DEVICE).eval()\n",
    "patch_size = model.patch_size\n",
    "embed_dim = model.embed_dim\n",
    "print('Model:', MODEL_NAME, 'patch_size=', patch_size, 'embed_dim=', embed_dim)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "SQWsUEJ3_Jo4",
   "metadata": {
    "id": "SQWsUEJ3_Jo4"
   },
   "source": [
    "## Helper transforms and small utilities\n",
    "ResizeToMultiple ensures both sides are multiples of patch_size and the short side is around SHORT_SIDE."
   ]
  },
  {
   "cell_type": "code",
   "id": "ZdFjMF2h_Jo4",
   "metadata": {
    "id": "ZdFjMF2h_Jo4"
   },
   "source": [
    "class ResizeToMultiple(nn.Module):\n",
    "    def __init__(self, short_side: int, multiple: int):\n",
    "        super().__init__()\n",
    "        self.short_side = short_side\n",
    "        self.multiple = multiple\n",
    "\n",
    "    def _round_up(self, side: float) -> int:\n",
    "        return math.ceil(side / self.multiple) * self.multiple\n",
    "\n",
    "    def forward(self, img):\n",
    "        old_width, old_height = TVTF.get_image_size(img)\n",
    "        if old_width > old_height:\n",
    "            new_height = self._round_up(self.short_side)\n",
    "            new_width = self._round_up(old_width * new_height / old_height)\n",
    "        else:\n",
    "            new_width = self._round_up(self.short_side)\n",
    "            new_height = self._round_up(old_height * new_width / old_width)\n",
    "        return TVTF.resize(img, [new_height, new_width], interpolation=TVT.InterpolationMode.BICUBIC)\n",
    "\n",
    "transform = TVT.Compose([\n",
    "    ResizeToMultiple(short_side=SHORT_SIDE, multiple=patch_size),\n",
    "    TVT.ToTensor(),\n",
    "    TVT.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "@torch.no_grad()\n",
    "def forward_feats(img_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"img_tensor: [3, H, W] normalized, on DEVICE -> returns [h, w, D]\"\"\"\n",
    "    feats = model.get_intermediate_layers(img_tensor.unsqueeze(0), n=1, reshape=True)[0]  # [1, D, h, w]\n",
    "    feats = feats.movedim(-3, -1)  # [1, h, w, D]\n",
    "    feats = F.normalize(feats, dim=-1, p=2)\n",
    "    return feats.squeeze(0)\n",
    "\n",
    "def make_neighborhood_mask(h: int, w: int, size: float, shape: str = 'circle') -> torch.Tensor:\n",
    "    ij = torch.stack(\n",
    "        torch.meshgrid(\n",
    "            torch.arange(h, dtype=torch.float32, device=DEVICE),\n",
    "            torch.arange(w, dtype=torch.float32, device=DEVICE),\n",
    "            indexing='ij',\n",
    "        ),\n",
    "        dim=-1,\n",
    "    )\n",
    "    if shape == 'circle':\n",
    "        ord = 2\n",
    "    elif shape == 'square':\n",
    "        ord = torch.inf\n",
    "    else:\n",
    "        raise ValueError(f'Invalid shape={shape}')\n",
    "    norm = torch.linalg.vector_norm(\n",
    "        ij[:, :, None, None, :] - ij[None, None, :, :, :],  # [h\", w\", h, w, 2]\n",
    "        ord=ord,\n",
    "        dim=-1,\n",
    "    )\n",
    "    mask = norm <= size\n",
    "    return mask\n",
    "\n",
    "@torch.no_grad()\n",
    "def propagate(current_features: torch.Tensor,\n",
    "              context_features: torch.Tensor,\n",
    "              context_probs: torch.Tensor,\n",
    "              neighborhood_mask: torch.Tensor,\n",
    "              topk: int,\n",
    "              temperature: float) -> torch.Tensor:\n",
    "    \"\"\"Propagate context_probs -> returns [h2, w2, M]\"\"\"\n",
    "    t, h, w, M = context_probs.shape\n",
    "    # similarity\n",
    "    dot = torch.einsum('ijd, tuvd -> ijtuv', current_features, context_features)  # [h2, w2, t, h, w]\n",
    "    dot = torch.where(neighborhood_mask[:, :, None, :, :], dot, -torch.inf)\n",
    "    dotflat = dot.flatten(2, -1).flatten(0, 1)  # [h2*w2, t*h*w]\n",
    "    # safe topk: if t*h*w < topk reduce\n",
    "    k = min(topk, dotflat.shape[1])\n",
    "    if k <= 0:\n",
    "        raise RuntimeError('Empty context for propagation')\n",
    "    kth = torch.topk(dotflat, k=k, dim=1).values[:, -1:]\n",
    "    dotflat = torch.where(dotflat >= kth, dotflat, -torch.inf)\n",
    "    weights = torch.softmax(dotflat / temperature, dim=1)\n",
    "    context_flat = context_probs.flatten(0, 2)  # [t*h*w, M]\n",
    "    pred = weights @ context_flat\n",
    "    pred = pred / (pred.sum(dim=1, keepdim=True) + 1e-12)\n",
    "    return pred.unflatten(0, (current_features.shape[0], current_features.shape[1]))  # [h2, w2, M]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "_ynD0rLi_Jo4",
   "metadata": {
    "id": "_ynD0rLi_Jo4"
   },
   "source": [
    "## Load dataset (train.pkl) and inspect one sample\n",
    "Adjust path to your train.pkl file. The notebook assumes train.pkl is a list of dicts as described."
   ]
  },
  {
   "cell_type": "code",
   "id": "SdSe0tno_Jo4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SdSe0tno_Jo4",
    "outputId": "e21a1981-b502-4452-a46f-d9a7c4212373"
   },
   "source": [
    "# Path to data - change as needed\n",
    "import gzip # Added import for gzip\n",
    "TRAIN_PKL = '/content/drive/MyDrive/train.pkl'\n",
    "if not os.path.exists(TRAIN_PKL):\n",
    "    print('train.pkl not found in notebook directory. Please place train.pkl or change TRAIN_PKL path.')\n",
    "else:\n",
    "    try:\n",
    "        with gzip.open(TRAIN_PKL, 'rb') as f: # Changed to gzip.open\n",
    "            train_data = pickle.load(f)\n",
    "        print('Loaded train.pkl, number of videos:', len(train_data))\n",
    "        # show structure of first entry\n",
    "        sample = train_data[0]\n",
    "        print('Keys in sample:', list(sample.keys()))\n",
    "        print('Video shape (H,W,T):', sample['video'].shape)\n",
    "        print('Frames with labels:', sample.get('frames'))\n",
    "        print(\"Dataset source:\", sample.get('dataset'))\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading train.pkl: {e}\")\n",
    "        print(\"It might not be a gzipped pickle file, or the file is corrupted.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "gLVkxKJW_Jo5",
   "metadata": {
    "id": "gLVkxKJW_Jo5"
   },
   "source": [
    "## Core processing: create dense masks for a single video\n",
    "Function process_video_dict runs the pipeline for a single video dictionary from train.pkl.\n",
    "- Applies bbox crop (if present and enabled)\n",
    "- Converts grayscale->RGB by stacking\n",
    "- Computes features for all frames once\n",
    "- Uses the 3 annotated frames as context (one-hot) and propagates to all frames\n",
    "- Upsamples to cropped resolution and reinserts into full frame\n",
    "- Returns list of masks (H, W) per frame"
   ]
  },
  {
   "cell_type": "code",
   "id": "EWioCBHj_Jo5",
   "metadata": {
    "id": "EWioCBHj_Jo5"
   },
   "source": [
    "def process_video_dict(vd: dict,\n",
    "                       topk=TOPK,\n",
    "                       temp=TEMPERATURE,\n",
    "                       radius=NEIGHBORHOOD_PATCH_RADIUS,\n",
    "                       use_bbox=USE_BBOX_CROP):\n",
    "    \"\"\"Process one video dict from train.pkl; returns list of HxW masks (uint8) and confidence maps.\n",
    "    vd keys: 'name', 'video' (H,W,T), 'box' (H,W) bool, 'label' (H,W,T) or labels and 'frames'\n",
    "    \"\"\"\n",
    "    video = vd['video']  # shape H,W,T, dtype uint8\n",
    "    H, W, T = video.shape\n",
    "    # bbox crop\n",
    "    if use_bbox and ('box' in vd) and (vd['box'] is not None):\n",
    "        box = vd['box']\n",
    "        ys, xs = np.where(box)\n",
    "        y0, y1 = int(ys.min()), int(ys.max()) + 1\n",
    "        x0, x1 = int(xs.min()), int(xs.max()) + 1\n",
    "    else:\n",
    "        y0, y1, x0, x1 = 0, H, 0, W\n",
    "    cropped = video[y0:y1, x0:x1, :]\n",
    "    # build PIL frames RGB from grayscale\n",
    "    pil_frames = []\n",
    "    for t in range(T):\n",
    "        im = Image.fromarray(cropped[..., t]).convert('L')\n",
    "        arr = np.array(im)\n",
    "        rgb = np.stack([arr, arr, arr], axis=-1).astype(np.uint8)\n",
    "        pil_frames.append(Image.fromarray(rgb))\n",
    "    # compute features for all frames (may be slow)\n",
    "    feats_list = []\n",
    "    for t in range(T):\n",
    "        inp = transform(pil_frames[t]).to(DEVICE)\n",
    "        feats = forward_feats(inp)  # [h, w, D]\n",
    "        feats_list.append(feats)\n",
    "    feats = torch.stack(feats_list, dim=0)  # [T, h, w, D]\n",
    "    h, w = feats.shape[1], feats.shape[2]\n",
    "    # build context from annotated frames\n",
    "    frames_annot = vd.get('frames', [])\n",
    "    if len(frames_annot) == 0:\n",
    "        raise RuntimeError('No annotated frames found in video dict')\n",
    "    context_idx = frames_annot\n",
    "    context_features = feats[context_idx]  # [t, h, w, D]\n",
    "    # build context_probs from label masks\n",
    "    labels = vd.get('label', None)\n",
    "    if labels is None:\n",
    "        raise RuntimeError('No label key in training datum')\n",
    "    # labels expected shape: H, W, maybe T or list: handle both\n",
    "    context_probs_list = []\n",
    "    M = 2 if ASSUME_BINARY else int(labels.max() + 1)\n",
    "    for idx in context_idx:\n",
    "        lab_full = labels[..., idx]\n",
    "        lab_crop = lab_full[y0:y1, x0:x1].astype(np.int64)\n",
    "        lab_t = torch.from_numpy(lab_crop).to(DEVICE)\n",
    "        lab_grid = F.interpolate(lab_t[None, None].float(), (h, w), mode='nearest-exact')[0, 0].long()\n",
    "        onehot = F.one_hot(lab_grid, num_classes=M).float()  # [h, w, M]\n",
    "        context_probs_list.append(onehot)\n",
    "    context_probs = torch.stack(context_probs_list, dim=0)  # [t, h, w, M]\n",
    "    # neighborhood mask (same grid dims)\n",
    "    neighborhood_mask = make_neighborhood_mask(h, w, radius, shape='circle')\n",
    "    # propagate to all frames\n",
    "    preds_per_frame = []\n",
    "    confidences = []\n",
    "    for t in range(T):\n",
    "        cur_feats = feats[t]\n",
    "        pred_grid = propagate(cur_feats, context_features, context_probs, neighborhood_mask, topk, temp)\n",
    "        # pred_grid [h, w, M]\n",
    "        # record confidence (max prob per patch)\n",
    "        conf_patch = pred_grid.max(dim=-1).values.cpu().numpy()  # [h,w]\n",
    "        conf = torch.from_numpy(conf_patch)[None, None].float()  # [1,1,h,w]\n",
    "        conf_up = F.interpolate(conf, size=(y1-y0, x1-x0), mode='bilinear', align_corners=False)[0,0].numpy()\n",
    "        confidences.append(conf_up)\n",
    "        # upsample to crop resolution\n",
    "        prob = pred_grid.permute(2, 0, 1).unsqueeze(0)  # [1, M, h, w]\n",
    "        up = F.interpolate(prob, size=(y1 - y0, x1 - x0), mode='bilinear', align_corners=False)[0]  # [M, Hcrop, Wcrop]\n",
    "        mask_crop = up.argmax(0).cpu().numpy().astype(np.uint8)\n",
    "        # reinsert into full frame\n",
    "        full_mask = np.zeros((H, W), dtype=np.uint8)\n",
    "        full_mask[y0:y1, x0:x1] = mask_crop\n",
    "        preds_per_frame.append(full_mask)\n",
    "    # return per-frame masks and confidences (per-patch upsampled confidence can also be saved)\n",
    "    return preds_per_frame, confidences\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "@torch.no_grad()\n",
    "def dino_mask_feature_for_frame(vd, t, mask_full,\n",
    "                                use_bbox=USE_BBOX_CROP) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    vd: one entry from train_data (with 'video' and optional 'box')\n",
    "    t: frame index\n",
    "    mask_full: [H, W] uint8 or bool mask (full-frame prediction)\n",
    "    returns: 1D numpy vector of size embed_dim (mean DINO feature over mask)\n",
    "    \"\"\"\n",
    "    video = vd[\"video\"]  # [H, W, T], uint8\n",
    "    H, W, T = video.shape\n",
    "    assert 0 <= t < T\n",
    "\n",
    "    # --- same crop as in process_video_dict ---\n",
    "    if use_bbox and (\"box\" in vd) and (vd[\"box\"] is not None):\n",
    "        box = vd[\"box\"]  # [H, W] bool\n",
    "        ys, xs = np.where(box)\n",
    "        y0, y1 = int(ys.min()), int(ys.max()) + 1\n",
    "        x0, x1 = int(xs.min()), int(xs.max()) + 1\n",
    "    else:\n",
    "        y0, y1, x0, x1 = 0, H, 0, W\n",
    "\n",
    "    frame_crop = video[y0:y1, x0:x1, t]       # [Hc, Wc]\n",
    "    mask_crop  = mask_full[y0:y1, x0:x1]      # [Hc, Wc]\n",
    "\n",
    "    # --- build RGB PIL (like process_video_dict) ---\n",
    "    im = Image.fromarray(frame_crop).convert(\"L\")\n",
    "    arr = np.array(im)\n",
    "    rgb = np.stack([arr, arr, arr], axis=-1).astype(np.uint8)\n",
    "    pil = Image.fromarray(rgb)\n",
    "\n",
    "    # --- DINO features: [h, w, D] ---\n",
    "    inp = transform(pil).to(DEVICE)          # [3, H', W']\n",
    "    feats = forward_feats(inp)               # [h, w, D]\n",
    "    h, w, D = feats.shape\n",
    "\n",
    "    # --- map mask to patch grid ---\n",
    "    mask_t = torch.from_numpy(mask_crop.astype(np.float32))[None, None]  # [1,1,Hc,Wc]\n",
    "    mask_small = F.interpolate(mask_t, size=(h, w), mode=\"nearest\")[0, 0]  # [h, w]\n",
    "    mask_bool = mask_small > 0.5\n",
    "\n",
    "    # if mask is empty after downsampling, fall back to global average\n",
    "    if mask_bool.sum() == 0:\n",
    "        vec = feats.mean(dim=(0, 1))\n",
    "    else:\n",
    "        vec = feats[mask_bool].mean(dim=0)    # [D]\n",
    "\n",
    "    return vec.cpu().numpy()\n"
   ],
   "metadata": {
    "id": "UHr1S12l2KTD"
   },
   "id": "UHr1S12l2KTD",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "gaYimTli_Jo5",
   "metadata": {
    "id": "gaYimTli_Jo5"
   },
   "source": [
    "## Demo on a single training video (if train.pkl present)\n",
    "This cell runs the pipeline for the first training video and visualizes results for a few frames."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import Image as IPyImage, display\n",
    "\n",
    "def make_overlay_gif(vd, preds, gif_path, fps=20, alpha=0.4):\n",
    "    \"\"\"\n",
    "    vd: dict from train_data (with 'video' key: H x W x T)\n",
    "    preds: list/array of masks for each frame (len = T)\n",
    "    gif_path: where to save the GIF\n",
    "    \"\"\"\n",
    "    video = vd['video']  # H, W, T\n",
    "    H, W, T = video.shape\n",
    "\n",
    "    frames = []\n",
    "    for t in range(T):\n",
    "        img = video[..., t]          # H x W\n",
    "        mask = preds[t]              # H x W\n",
    "\n",
    "        # ensure uint8 grayscale\n",
    "        img = img.astype(np.uint8)\n",
    "\n",
    "        # binary mask\n",
    "        mask_bin = (mask > 0).astype(np.uint8)\n",
    "\n",
    "        # grayscale -> RGB\n",
    "        img_rgb = np.stack([img, img, img], axis=-1).astype(np.uint8)\n",
    "\n",
    "        # red overlay where mask == 1\n",
    "        overlay = img_rgb.copy()\n",
    "        overlay[mask_bin == 1] = [255, 0, 0]  # RGB red\n",
    "\n",
    "        out_rgb = (img_rgb * (1 - alpha) + overlay * alpha).astype(np.uint8)\n",
    "        frames.append(Image.fromarray(out_rgb))\n",
    "\n",
    "    # save GIF\n",
    "    frames[0].save(\n",
    "        gif_path,\n",
    "        save_all=True,\n",
    "        append_images=frames[1:],\n",
    "        duration=int(1000 / fps),  # ms per frame\n",
    "        loop=0,\n",
    "    )\n",
    "\n",
    "    return gif_path"
   ],
   "metadata": {
    "id": "2cvKrBlZp0Ph"
   },
   "id": "2cvKrBlZp0Ph",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eniu6GPL_Jo5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 934
    },
    "id": "eniu6GPL_Jo5",
    "outputId": "cd535545-dc2b-444a-be2f-8a25b7192849"
   },
   "source": [
    "if 'train_data' in globals():\n",
    "    n_samples = min(5, len(train_data))\n",
    "    print(f\"Processing {n_samples} videos from train_data\")\n",
    "\n",
    "    for idx in range(n_samples):\n",
    "        vd = train_data[idx]\n",
    "        name = vd.get('name', f'sample_{idx}')\n",
    "\n",
    "        print(f\"\\n[{idx+1}/{n_samples}] Processing video '{name}'\")\n",
    "        start = time.time()\n",
    "        preds, confs = process_video_dict(vd)\n",
    "        print('  Done in', time.time() - start, 's, produced', len(preds), 'frames')\n",
    "\n",
    "        gif_path = f\"{name}_overlay.gif\"\n",
    "        make_overlay_gif(vd, preds, gif_path, fps=20, alpha=0.4)\n",
    "        print(\"  Saved GIF to\", gif_path)\n",
    "\n",
    "        # show the GIF inline\n",
    "        display(IPyImage(filename=gif_path))\n",
    "\n",
    "else:\n",
    "    print('No train.pkl loaded; skip demo.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "Rf3oLMQ6_Jo5",
   "metadata": {
    "id": "Rf3oLMQ6_Jo5"
   },
   "source": [
    "## Save pseudo-labels for all training videos\n",
    "We save per-video per-frame masks as compressed numpy .npz files. You can then use these to train a segmentation model."
   ]
  },
  {
   "cell_type": "code",
   "id": "ivxF9w_h_Jo6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ivxF9w_h_Jo6",
    "outputId": "932591d3-a722-4d51-e90b-e7d0a6dbe33e"
   },
   "source": [
    "if 'train_data' in globals():\n",
    "    out_dir = Path(SAVE_PSEUDOLABELS_DIR)\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "    for i, vd in enumerate(train_data):\n",
    "        name = vd.get('name', f'video_{i}')\n",
    "        print(f'[{i+1}/{len(train_data)}] processing', name)\n",
    "        try:\n",
    "            preds, confs = process_video_dict(vd)\n",
    "            # save masks as uint8 per-frame stack\n",
    "            arr = np.stack(preds, axis=-1).astype(np.uint8)  # H,W,T\n",
    "            # confidences as float32 stack\n",
    "            conf_arr = np.stack(confs, axis=-1).astype(np.float32)\n",
    "            np.savez_compressed(out_dir / f'{name}.npz', masks=arr, conf=conf_arr)\n",
    "        except Exception as e:\n",
    "            print('Error on', name, e)\n",
    "    print('Saved pseudo-labels to', out_dir)\n",
    "else:\n",
    "    print('train.pkl not loaded. Skipping saving pseudo-labels.')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ivgvQeBWVkVw",
   "metadata": {
    "id": "ivgvQeBWVkVw"
   },
   "source": [
    "!cp -r /content/pseudo_labels /content/drive/MyDrive/"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "gjkUsd3aWCMP",
   "metadata": {
    "id": "gjkUsd3aWCMP"
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "pseudo_dir = SAVE_PSEUDOLABELS_DIR  # e.g. \"data/pseudo_labels\"\n",
    "all_feats = []\n",
    "\n",
    "# map name -> vd so we can recover the original video for each npz file\n",
    "name_to_vd = {}\n",
    "for vd in train_data:\n",
    "    name = vd.get(\"name\", None)\n",
    "    if name is not None:\n",
    "        name_to_vd[name] = vd\n",
    "\n",
    "for fname in os.listdir(pseudo_dir):\n",
    "    if not fname.endswith(\".npz\"):\n",
    "        continue\n",
    "\n",
    "    name = fname.split(\".\")[0]\n",
    "    if name not in name_to_vd:\n",
    "        print(f\"Warning: no vd for {name}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    vd = name_to_vd[name]\n",
    "    npz = np.load(os.path.join(pseudo_dir, fname))\n",
    "    masks = npz[\"masks\"]  # [H, W, T]\n",
    "    confs = npz[\"conf\"]   # [H, W, T]\n",
    "    H, W, T = masks.shape\n",
    "\n",
    "    for t in range(T):\n",
    "        # optional: only use reasonably confident frames for training the GMM\n",
    "        if confs[..., t].mean() < 0.2:\n",
    "            continue\n",
    "\n",
    "        vec = dino_mask_feature_for_frame(vd, t, masks[..., t])\n",
    "        all_feats.append(vec)\n",
    "\n",
    "all_feats = np.stack(all_feats, axis=0)  # [N_frames, D]\n",
    "print(\"DINO mask features shape:\", all_feats.shape)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# --- standardize ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(all_feats)\n",
    "\n",
    "# --- PCA to reduce dimension (tune n_components) ---\n",
    "pca = PCA(n_components=32, random_state=0)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# --- GMM on PCA space ---\n",
    "gmm = GaussianMixture(\n",
    "    n_components=3,          # tune (2–5 usually fine)\n",
    "    covariance_type=\"full\",\n",
    "    random_state=0,\n",
    ")\n",
    "gmm.fit(X_pca)\n",
    "\n",
    "# log-likelihood for training points\n",
    "logp = gmm.score_samples(X_pca)\n",
    "print(\"logp range:\", logp.min(), logp.max())\n",
    "\n",
    "# choose an anomaly threshold, e.g. keep 95% most likely\n",
    "threshold = np.percentile(logp, 5)\n",
    "print(\"anomaly threshold:\", threshold)\n"
   ],
   "metadata": {
    "id": "zlggaG1G2cUq"
   },
   "id": "zlggaG1G2cUq",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def dino_gmm_anomaly_score(vd, t, mask_full):\n",
    "    vec = dino_mask_feature_for_frame(vd, t, mask_full)\n",
    "    Xs = scaler.transform(vec[None, :])\n",
    "    Xp = pca.transform(Xs)\n",
    "    logp = gmm.score_samples(Xp)[0]\n",
    "    return logp\n",
    "\n",
    "def is_outlier(vd, t, mask_full):\n",
    "    logp = dino_gmm_anomaly_score(vd, t, mask_full)\n",
    "    return logp < threshold\n"
   ],
   "metadata": {
    "id": "X79tSB8l2qUe"
   },
   "id": "X79tSB8l2qUe",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "self.items = []\n",
    "for vd in train_data:\n",
    "    name = vd.get(\"name\", None)\n",
    "    npz_path = os.path.join(SAVE_PSEUDOLABELS_DIR, f\"{name}.npz\")\n",
    "    if not os.path.exists(npz_path):\n",
    "        continue\n",
    "    data = np.load(npz_path)\n",
    "    masks = data[\"masks\"]  # [H, W, T]\n",
    "    confs = data[\"conf\"]\n",
    "\n",
    "    H, W, T = masks.shape\n",
    "    for t in range(T):\n",
    "        # optional: basic filters first\n",
    "        if confs[..., t].mean() < 0.2:\n",
    "            continue\n",
    "\n",
    "        # GMM filter\n",
    "        logp = dino_gmm_anomaly_score(vd, t, masks[..., t])\n",
    "        if logp < threshold:\n",
    "            continue  # skip anomalous frame\n",
    "\n",
    "        self.items.append((vd, t))\n"
   ],
   "metadata": {
    "id": "vzG-tTS42t-E"
   },
   "id": "vzG-tTS42t-E",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "nncCHonf_Jo6",
   "metadata": {
    "id": "nncCHonf_Jo6"
   },
   "source": [
    "## (Optional) Training a segmentation network on pseudo-labels\n",
    "Below is a compact skeleton for training a UNet-like model. It's intentionally short — replace with your preferred architecture and training loop. Use strong data augmentation and validation on expert-labeled subset."
   ]
  },
  {
   "cell_type": "code",
   "id": "WuQQAnsg_Jo6",
   "metadata": {
    "id": "WuQQAnsg_Jo6"
   },
   "source": [
    "class SmallUNet(nn.Module):\n",
    "    # very small UNet skeleton for demonstration; replace with proven arch\n",
    "    def __init__(self, in_ch=3, out_ch=2):\n",
    "        super().__init__()\n",
    "        self.enc1 = nn.Sequential(nn.Conv2d(in_ch, 32, 3, padding=1), nn.ReLU(), nn.Conv2d(32,32,3,padding=1), nn.ReLU())\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.enc2 = nn.Sequential(nn.Conv2d(32,64,3,padding=1), nn.ReLU(), nn.Conv2d(64,64,3,padding=1), nn.ReLU())\n",
    "        self.up = nn.ConvTranspose2d(64,32,2,stride=2)\n",
    "        self.dec1 = nn.Sequential(nn.Conv2d(64,32,3,padding=1), nn.ReLU(), nn.Conv2d(32,32,3,padding=1), nn.ReLU())\n",
    "        self.head = nn.Conv2d(32,out_ch,1)\n",
    "    def forward(self,x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        u = self.up(e2)\n",
    "        d = self.dec1(torch.cat([u,e1], dim=1))\n",
    "        return self.head(d)\n",
    "\n",
    "# NOTE: training loop is left minimal. For a real experiment, implement dataset loader, augmentation,\n",
    "# optimizer, scheduler, proper validation and checkpointing. We intentionally do not run training here.\n",
    "\n",
    "print('UNet skeleton ready (not training in notebook by default).')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "F-Zw5cvS_Jo6",
   "metadata": {
    "id": "F-Zw5cvS_Jo6"
   },
   "source": [
    "## Notes, tips and next steps\n",
    "- If DINOv3 features do not match well on echocardiography, try preprocessing: CLAHE, histogram equalization, contrast adjustments, or a small learned colorization network before DINOv3.\n",
    "- If memory is an issue, reduce SHORT_SIDE or use a smaller DINOv3 model (dinov3_vits16).\n",
    "- To improve robustness, use all 3 annotated frames as context (done above), and optionally perform forward/backward propagation and average results.\n",
    "- When creating pseudo-labels for training, filter by confidence per-frame or per-pixel, and prefer expert-labeled videos or give them higher weight.\n",
    "- For test inference, either: (A) run propagation with the provided 3 context frames (if test has context), or (B) use a trained segmentation model for speed and consistency.\n",
    "\n",
    "If you want, I can now:\n",
    "- 1) adapt this notebook to run distributed or tiled propagation for very large crops to save memory, or\n",
    "- 2) implement the full training loop + data loader that consumes the generated pseudo-labels and trains a UNet with augmentation, or\n",
    "- 3) add detailed visualization utilities (top-k patch matches, per-patch similarity heatmaps) to debug propagation failures.\n",
    "\n",
    "Which would you like next?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "title": "Mitral Valve segmentation with DINOv3 - propagation + optional self-training"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
